{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import python files from another directory\n",
    "In order to reuse code written in .py files you need to import them.\n",
    "If they are from a distant folder, rather than copy-paste it into the current folder, you can write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\n",
    "    \"../../ex02/template\"\n",
    ")  # This goes up twice in the directories tree (hence in labs)\n",
    "# then goes down to ex02/template where your files from lab 2 are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can now import your desired files, for example, we can import grid_search.py with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100.  -25.   50.  125.  200.] [-150.  -75.    0.   75.  150.]\n"
     ]
    }
   ],
   "source": [
    "import grid_search  # You then need to call your functions using grid_search.function_name()\n",
    "import grid_search as gs  # You then need to call your functions using gs.function_name()\n",
    "from grid_search import *  # You can call any functions of the file with function_name()\n",
    "\n",
    "# Let's try to call generate_w from grid_search.py:\n",
    "w0, w1 = generate_w(5)\n",
    "print(w0, w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we are now able to call functions from the grid_search.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "\n",
    "from test_utils import test\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Least squares and linear basis functions models\n",
    "## 1.1 Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"Calculate the least squares solution.\n",
    "       returns mse, and optimal weights.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape (N,), N is the number of samples.\n",
    "        tx: numpy array of shape (N,D), D is the number of features.\n",
    "\n",
    "    Returns:\n",
    "        w: optimal weights, numpy array of shape(D,), D is the number of features.\n",
    "        mse: scalar.\n",
    "\n",
    "    >>> least_squares(np.array([0.1,0.2]), np.array([[2.3, 3.2], [1., 0.1]]))\n",
    "    (array([ 0.21212121, -0.12121212]), 8.666684749742561e-33)\n",
    "    \"\"\"\n",
    "    # Normal equation: w = (X^T X)^-1 X^T y\n",
    "    w = np.linalg.solve(tx.T @ tx, tx.T @ y)\n",
    "    \n",
    "    # Compute the residuals: e = y - Xw\n",
    "    e = y - tx @ w\n",
    "    \n",
    "    # Compute the Mean Squared Error (MSE)\n",
    "    mse = np.mean(e**2) / 2\n",
    "    \n",
    "    return w, float(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can test your implementation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Your `least_squares` passed 1 tests.\n"
     ]
    }
   ],
   "source": [
    "test(least_squares)\n",
    "\n",
    "\n",
    "# NB:\n",
    "#\n",
    "# Due to precision issues,\n",
    "# the output might not be exactly the same\n",
    "# even if you have implemented the right code.\n",
    "#\n",
    "# For example, the mse output expected to be\n",
    "# 8.666684749742561e-33,\n",
    "# but you might get some other small number\n",
    "# close to zero.\n",
    "#\n",
    "# In this case,\n",
    "# Failing the test doesn't necessarily mean\n",
    "# your implementation is wrong.:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "Here we will reuse the dataset `height_weight_genders.csv` from previous exercise section to check the correctness of your implementation. Please compare it with your previous result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_descent import gradient_descent\n",
    "from costs import compute_loss\n",
    "\n",
    "def test_your_least_squares():\n",
    "    \"\"\"\n",
    "    This code compares the optimal weights obtained\n",
    "    by least squares vs. gradient descent.\n",
    "    \"\"\"\n",
    "    height, weight, gender = load_data_from_ex02(sub_sample=False, add_outlier=False)\n",
    "    x, mean_x, std_x = standardize(height)\n",
    "    y, tx = build_model_data(x, weight)\n",
    "\n",
    "    # Perform least squares\n",
    "    w_ls, mse_ls = least_squares(y, tx)\n",
    "\n",
    "    print(\"Least Squares Results:\")\n",
    "    print(\"Optimal weights (w_ls):\", w_ls)\n",
    "    print(\"MSE (mse_ls):\", mse_ls)\n",
    "\n",
    "    # Define parameters for gradient descent\n",
    "    max_iters = 50\n",
    "    gamma = 0.7\n",
    "    initial_w = np.array([0.0, 0.0])\n",
    "\n",
    "    # Perform gradient descent\n",
    "    losses_gd, ws_gd = gradient_descent(y, tx, initial_w, max_iters, gamma)\n",
    "\n",
    "    # Take the final weights and loss from gradient descent\n",
    "    w_gd = ws_gd[-1]\n",
    "    mse_gd = losses_gd[-1]\n",
    "\n",
    "    # Print gradient descent results\n",
    "    print(\"\\nGradient Descent Results:\")\n",
    "    print(\"Optimal weights (w_gd):\", w_gd)\n",
    "    print(\"MSE (mse_gd):\", mse_gd)\n",
    "\n",
    "    # Compare the results\n",
    "    print(\"\\nComparison:\")\n",
    "    print(\"Difference in weights:\", w_ls - w_gd)\n",
    "    print(\"Difference in MSE:\", mse_ls - mse_gd)\n",
    "\n",
    "    # least square or grid search: TODO\n",
    "    # this code should compare the optimal weights obtained\n",
    "    # by least squares vs. grid search\n",
    "    # ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least Squares Results:\n",
      "Optimal weights (w_ls): [73.293922   13.47971243]\n",
      "MSE (mse_ls): 15.3858878688294\n",
      "GD iter. 0/49: loss=2792.2367127591674, w0=51.30574540147368, w1=9.435798704492347\n",
      "GD iter. 1/49: loss=265.3024621089587, w0=66.69746902191575, w1=12.266538315840018\n",
      "GD iter. 2/49: loss=37.87837955044096, w0=71.31498610804836, w1=13.115760199244335\n",
      "GD iter. 3/49: loss=17.41021212017444, w0=72.70024123388815, w1=13.370526764265632\n",
      "GD iter. 4/49: loss=15.568077051450448, w0=73.11581777164008, w1=13.446956733772025\n",
      "GD iter. 5/49: loss=15.402284895265295, w0=73.24049073296565, w1=13.469885724623941\n",
      "GD iter. 6/49: loss=15.38736360120863, w0=73.27789262136334, w1=13.476764421879516\n",
      "GD iter. 7/49: loss=15.38602068474353, w0=73.28911318788263, w1=13.478828031056189\n",
      "GD iter. 8/49: loss=15.385899822261674, w0=73.29247935783842, w1=13.47944711380919\n",
      "GD iter. 9/49: loss=15.385888944638305, w0=73.29348920882515, w1=13.47963283863509\n",
      "GD iter. 10/49: loss=15.3858879656522, w0=73.29379216412117, w1=13.479688556082861\n",
      "GD iter. 11/49: loss=15.385887877543452, w0=73.29388305071, w1=13.479705271317192\n",
      "GD iter. 12/49: loss=15.385887869613667, w0=73.29391031668663, w1=13.479710285887492\n",
      "GD iter. 13/49: loss=15.385887868899983, w0=73.29391849647962, w1=13.479711790258582\n",
      "GD iter. 14/49: loss=15.38588786883575, w0=73.29392095041752, w1=13.479712241569908\n",
      "GD iter. 15/49: loss=15.385887868829974, w0=73.29392168659889, w1=13.479712376963306\n",
      "GD iter. 16/49: loss=15.38588786882945, w0=73.2939219074533, w1=13.479712417581325\n",
      "GD iter. 17/49: loss=15.385887868829403, w0=73.29392197370963, w1=13.479712429766732\n",
      "GD iter. 18/49: loss=15.3858878688294, w0=73.29392199358652, w1=13.479712433422353\n",
      "GD iter. 19/49: loss=15.385887868829403, w0=73.2939219995496, w1=13.47971243451904\n",
      "GD iter. 20/49: loss=15.385887868829398, w0=73.29392200133852, w1=13.479712434848047\n",
      "GD iter. 21/49: loss=15.3858878688294, w0=73.29392200187519, w1=13.479712434946748\n",
      "GD iter. 22/49: loss=15.3858878688294, w0=73.2939220020362, w1=13.479712434976358\n",
      "GD iter. 23/49: loss=15.3858878688294, w0=73.29392200208449, w1=13.479712434985242\n",
      "GD iter. 24/49: loss=15.3858878688294, w0=73.29392200209898, w1=13.479712434987906\n",
      "GD iter. 25/49: loss=15.385887868829398, w0=73.29392200210333, w1=13.479712434988706\n",
      "GD iter. 26/49: loss=15.3858878688294, w0=73.29392200210464, w1=13.479712434988945\n",
      "GD iter. 27/49: loss=15.3858878688294, w0=73.29392200210502, w1=13.479712434989018\n",
      "GD iter. 28/49: loss=15.3858878688294, w0=73.29392200210513, w1=13.47971243498904\n",
      "GD iter. 29/49: loss=15.3858878688294, w0=73.29392200210518, w1=13.479712434989047\n",
      "GD iter. 30/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 31/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 32/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 33/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 34/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 35/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 36/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 37/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 38/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 39/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 40/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 41/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 42/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 43/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 44/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 45/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 46/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 47/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 48/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 49/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "\n",
      "Gradient Descent Results:\n",
      "Optimal weights (w_gd): [73.293922   13.47971243]\n",
      "MSE (mse_gd): 15.3858878688294\n",
      "\n",
      "Comparison:\n",
      "Difference in weights: [-4.26325641e-13  2.36255460e-13]\n",
      "Difference in MSE: 0.0\n"
     ]
    }
   ],
   "source": [
    "test_your_least_squares()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Least squares with a linear basis function model\n",
    "Start from this section, we will use the dataset `dataEx3.csv`.\n",
    "\n",
    "### Implement polynomial basis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\n",
    "\n",
    "    Args:\n",
    "        x: numpy array of shape (N,), N is the number of samples.\n",
    "        degree: integer.\n",
    "\n",
    "    Returns:\n",
    "        poly: numpy array of shape (N,d+1)\n",
    "\n",
    "    >>> build_poly(np.array([0.0, 1.5]), 2)\n",
    "    array([[1.  , 0.  , 0.  ],\n",
    "           [1.  , 1.5 , 2.25]])\n",
    "    \"\"\"\n",
    "    return np.vstack([x**d for d in range(degree + 1)]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Your `build_poly` passed 1 tests.\n"
     ]
    }
   ],
   "source": [
    "test(build_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your implementation of `build_poly` passes the test, copy it to `build_polynomial.py`\n",
    "Let us play with polynomial regression. Note that we will use your implemented function `compute_mse`. Please copy and paste your implementation from exercise02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import *\n",
    "\n",
    "\n",
    "def polynomial_regression():\n",
    "    \"\"\"Constructing the polynomial basis function expansion of the data,\n",
    "    and then running least squares regression.\"\"\"\n",
    "    # define parameters\n",
    "    degrees = [1, 3, 7, 12]\n",
    "\n",
    "    # define the structure of the figure\n",
    "    num_row = 2\n",
    "    num_col = 2\n",
    "    f, axs = plt.subplots(num_row, num_col)\n",
    "\n",
    "    for ind, degree in enumerate(degrees):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # form the data to do polynomial regression.: TODO\n",
    "        # ***************************************************\n",
    "        raise NotImplementedError\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # least square and calculate RMSE: TODO\n",
    "        # ***************************************************\n",
    "        raise NotImplementedError\n",
    "\n",
    "        print(\n",
    "            \"Processing {i}th experiment, degree={d}, rmse={loss}\".format(\n",
    "                i=ind + 1, d=degree, loss=rmse\n",
    "            )\n",
    "        )\n",
    "        # plot fit\n",
    "        plot_fitted_curve(y, x, weights, degree, axs[ind // num_col][ind % num_col])\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your results should look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](visualize_polynomial_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Evaluating model predication performance\n",
    "\n",
    "Let us show the train and test splits for various polynomial degrees. First of all, please fill in the function `split_data()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio. If ratio is 0.8\n",
    "    you will have 80% of your data set dedicated to training\n",
    "    and the rest dedicated to testing. If ratio times the number of samples is not round\n",
    "    you can use np.floor. Also check the documentation for np.random.permutation,\n",
    "    it could be useful.\n",
    "\n",
    "    Args:\n",
    "        x: numpy array of shape (N,), N is the number of samples.\n",
    "        y: numpy array of shape (N,).\n",
    "        ratio: scalar in [0,1]\n",
    "        seed: integer.\n",
    "\n",
    "    Returns:\n",
    "        x_tr: numpy array containing the train data.\n",
    "        x_te: numpy array containing the test data.\n",
    "        y_tr: numpy array containing the train labels.\n",
    "        y_te: numpy array containing the test labels.\n",
    "\n",
    "    >>> split_data(np.arange(13), np.arange(13), 0.8, 1)\n",
    "    (array([ 2,  3,  4, 10,  1,  6,  0,  7, 12,  9]), array([ 8, 11,  5]), array([ 2,  3,  4, 10,  1,  6,  0,  7, 12,  9]), array([ 8, 11,  5]))\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # split the data based on the given ratio: TODO\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(split_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, test your `split_data` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"polynomial regression with different split ratios and different degrees.\n",
    "\n",
    "    Returns:\n",
    "      x_tr: numpy array\n",
    "      x_te: numpy array\n",
    "      y_tr: numpy array\n",
    "      y_te: numpy array\n",
    "      weights: weights from the least squares optimization\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # split the data, and return train and test data: TODO\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # form train and test data with polynomial basis function: TODO\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # calculate weight through least square: TODO\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # calculate RMSE for train and test data,\n",
    "    # and store them in rmse_tr and rmse_te respectively: TODO\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "    print(\n",
    "        \"proportion={p}, degree={d}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "            p=ratio, d=degree, tr=rmse_tr, te=rmse_te\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 6\n",
    "degrees = [1, 3, 7, 12]\n",
    "split_ratios = [0.9, 0.7, 0.5, 0.1]\n",
    "\n",
    "# define the structure of the figure\n",
    "num_row = 4\n",
    "num_col = 4\n",
    "axs = plt.subplots(num_row, num_col, figsize=(20, 8))[1]\n",
    "\n",
    "for ind, split_ratio in enumerate(split_ratios):\n",
    "    for ind_d, degree in enumerate(degrees):\n",
    "        x_tr, x_te, y_tr, y_te, w = train_test_split_demo(\n",
    "            x, y, degree, split_ratio, seed\n",
    "        )\n",
    "        plot_fitted_curve(y_tr, x_tr, w, degree, axs[ind_d][ind % num_col])\n",
    "        axs[ind_d][ind].set_title(f\"Degree: {degree}, Split {split_ratio}\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your graph should look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](split_demo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "Please fill in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape (N,), N is the number of samples.\n",
    "        tx: numpy array of shape (N,D), D is the number of features.\n",
    "        lambda_: scalar.\n",
    "\n",
    "    Returns:\n",
    "        w: optimal weights, numpy array of shape(D,), D is the number of features.\n",
    "\n",
    "    >>> ridge_regression(np.array([0.1,0.2]), np.array([[2.3, 3.2], [1., 0.1]]), 0)\n",
    "    array([ 0.21212121, -0.12121212])\n",
    "    >>> ridge_regression(np.array([0.1,0.2]), np.array([[2.3, 3.2], [1., 0.1]]), 1)\n",
    "    array([0.03947092, 0.00319628])\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(ridge_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-5, 0, 15)\n",
    "\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # split the data, and return train and test data: TODO\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # form train and test data with polynomial basis function: TODO\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # ridge regression with a given lambda\n",
    "        # ***************************************************\n",
    "        raise NotImplementedError\n",
    "        print(\n",
    "            \"proportion={p}, degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "                p=ratio, d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]\n",
    "            )\n",
    "        )\n",
    "    plot_train_test(rmse_tr, rmse_te, lambdas, degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Demo time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 56\n",
    "degree = 7\n",
    "split_ratio = 0.5\n",
    "ridge_regression_demo(x, y, degree, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your plot should look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](ridge_regression.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
